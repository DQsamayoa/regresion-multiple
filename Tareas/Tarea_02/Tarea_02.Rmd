---
title: "Regresión múltiple y otras técnicas multivariadas"
subtitle: "Tarea 02"
author:
  - "Rivera Torres Francisco de Jesús"
  - "Rodríguez Maya Jorge Daniel"
  - "Samayoa Donado Víctor Augusto"
  - "Trujillo Bariios Georgina"
date: "Febrero 20, 2019"
output:
  pdf_document:
    toc: false
    number_sections: false
    fig_caption: true
    highlight: kate
    df_print: kable
    includes:
      in_header: tex/header.tex
fontsize: 11pt
documentclass: article
classoption: twoside
fig_align: "center"
---

```{r setup, include = FALSE, eval = TRUE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, fig.height = 3)

# Se cargan las librerias a utilizar
library(tidyverse)
library(readxl)
library(scales)
library(grid)
library(kableExtra)
library(latex2exp)
```

# Ejercicio 1
Mostrar que los EMCO de $\beta_0$ y $\beta_1$ son lineales e insesgados.

# Ejercicio 2
Mostrar que $\hat{\mu}_0 = \hat{\beta}_1 + \hat{\beta}_1 x_0$ es un estimador insesgado de $\mu_0 = \beta_1 + \beta_1 x_0$ y que
\begin{align}
V(\hat{\mu}_0) = \sigma^2 \left(\dfrac{1}{n} + \dfrac{\left(x_0 - \bar{x} \right)^2}{S_{xx}} \right)
\end{align}

\begin{proof}
Procedemos a calcular el valor esperado de $\hat{\mu}_0$.
\begin{align*}
\mathrm{E} \left( \hat{\mu}_0 \right) &= \mathrm{E} \left(\hat{\beta}_0 + \hat{\beta}_1 x_0 \right) \\
& \text{aplicando la linealidad del la esperanza y considerando que} \\
& \text{$X$ es una variable NO aleatoria, se tiene} \\
&= \mathrm{E} \left( \hat{\beta}_0 \right) + x_0 \mathrm{E} \left(\hat{\beta}_1 \right) \\
& \text{pero por el ejercicio 1, sabemos que } \hat{\beta}_0 \text{ y } \hat{\beta}_0 \text{  son estimadores insesgados} \\
&= \beta_0 + \beta_1 x_0 = \mu_0
\end{align*}
por lo tanto se tiene que $\mathrm{E} (\hat{\mu}_0) = \mu_0$, lo cual significa que $\hat{\mu}_0$ es un estimador insesgado de $\mu_0$.

Por otro lado, la varianza está dada por:
\begin{align*}
V(\hat{\mu}_0)
&= \mathrm{E} \left( \hat{\mu}_0^2 \right) - \mathrm{E} \left( \hat{\mu}_0 \right)^2
= \mathrm{E} \left[ \left( \hat{\beta}_0 + \hat{\beta}_1 x_0 \right)^2 \right] - \mu_0^2
= \mathrm{E} \left( \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_0 + \hat{\beta}_1^2 x_0^2 \right) - \mu_0^2 \\
&= \mathrm{E} \left( \hat{\beta}_0^2 \right) + \mathrm{E} \left( 2 \hat{\beta}_0 \hat{\beta}_1 x_0 \right) + \mathrm{E} \left(\hat{\beta}_1^2 x_0^2 \right) - \mu_0^2 \\
& \text{considerando que } X \text{ es una variable NO aleatoria, se tiene} \\
&= \mathrm{E} \left( \hat{\beta}_0^2 \right) + 2 x_0 \mathrm{E} \left( \hat{\beta}_0 \hat{\beta}_1 \right) + x_0^2 \mathrm{E} \left(\hat{\beta}_1^2 \right) - \mu_0^2 \\
& \text{usando que } V(\hat{\beta}_i) = \mathrm{E}(\hat{\beta}_i^2) - \mathrm{E} (\hat{\beta}_i)^2, i = 0, 1 \text{, se tiene} \\
&= V \left( \hat{\beta}_0 \right) + \mathrm{E} \left( \hat{\beta}_0 \right)^2 + 2 x_0 \mathrm{E} \left( \hat{\beta}_0 \hat{\beta}_1 \right) + x_0^2\left( V \left(\hat{\beta}_1 \right) + \mathrm{E} \left(\hat{\beta}_1 \right)^2 \right) - \mu_0^2 \\
& \text{pero sabemos que los estimadores } \hat{\beta}_i, i = 0, 1 \text{ son insesgados, por lo que} \\
&= V \left( \hat{\beta}_0 \right) + \beta_0^2 + 2 x_0 \mathrm{E} \left( \hat{\beta}_0 \hat{\beta}_1 \right) + x_0^2\left( V \left(\hat{\beta}_1 \right) + \beta_1^2 \right) - \mu_0^2 \\
& \text{utilizando las expresiones para la varianza de } \hat{\beta}_i, i = 0, 1 \text{, se tiene que} \\
&= \left( \dfrac{1}{n} + \dfrac{\bar{x}^2}{S_{xx}} \right) \sigma^2 + \beta_0^2 + 2 x_0 \mathrm{E} \left( \hat{\beta}_0 \hat{\beta}_1 \right) + x_0^2\left( \dfrac{\sigma}{S_{xx}} + \beta_1^2 \right) - \mu_0^2 \\
&= \left( \dfrac{1}{n} + \dfrac{\bar{x}^2}{S_{xx}} \right) \sigma^2 + x_0^2 \dfrac{\sigma^2}{S_{xx}} + 2 x_0 \mathrm{E} \left( \hat{\beta}_0 \hat{\beta}_1 \right) - \left(\beta_0 + \beta_1 x_0 \right)^2  + \beta_0^2 + \beta_1^2 x_0^2 \\
&= \left( \dfrac{1}{n} + \dfrac{\bar{x}^2}{S_{xx}} \right) \sigma^2 + x_0^2 \dfrac{\sigma^2}{S_{xx}} + 2 x_0 \mathrm{E} \left( \hat{\beta}_0 \hat{\beta}_1 \right) - 2 \beta_0 \beta_1 x_0 \\
&= \left( \dfrac{1}{n} + \dfrac{\bar{x}^2}{S_{xx}} \right) \sigma^2 + x_0^2 \dfrac{\sigma^2}{S_{xx}} + 2 x_0 \left[ \mathrm{E} \left( \hat{\beta}_0 \hat{\beta}_1 \right) - \beta_0 \beta_1 \right] \\
& \text{utilizando el hecho de que los estimadores } \hat{\beta}_i, i = 0, 1 \text{ son insesgados, se tiene} \\
&= \left( \dfrac{1}{n} + \dfrac{\bar{x}^2}{S_{xx}} \right) \sigma^2 + x_0^2 \dfrac{\sigma^2}{S_{xx}} + 2 x_0 \left[\mathrm{E} \left( \hat{\beta}_0 \hat{\beta}_1 \right) - \mathrm{E} (\hat{\beta}_0) \mathrm{E} (\hat{\beta}_1) \right] \\
& \text{con esto obtenemos la expresión para la covarianza entre } \hat{\beta}_0 \text{ y } \hat{\beta}_1 \\
&= \left( \dfrac{1}{n} + \dfrac{\bar{x}^2}{S_{xx}} \right) \sigma^2 + x_0^2 \dfrac{\sigma^2}{S_{xx}} + 2 x_0 \mathrm{Cov} \left( \hat{\beta}_0, \hat{\beta}_1 \right) \\
& \text{utilizando la identidad para la covarianza de } \hat{\beta}_0 \text{ y } \hat{\beta}_1 \text{ se tiene que} \\
&= \left( \dfrac{1}{n} + \dfrac{\bar{x}^2}{S_{xx}} \right) \sigma^2 + x_0^2 \dfrac{\sigma^2}{S_{xx}} + 2 x_0 \left(-\dfrac{\bar{x}}{S_{xx}} \sigma^2 \right) \\
&= \left[ \dfrac{1}{n} + \dfrac{1}{S_{xx}} \left(\bar{x}^2 - 2 x_0 \bar{x} + x_0^2 \right) \right] \sigma^2
= \left[ \dfrac{1}{n} + \dfrac{1}{S_{xx}} \left(\bar{x} - x_0 \right)^2 \right] \sigma^2 \\
&= \left[ \dfrac{1}{n} + \dfrac{1}{S_{xx}} \left(x_0 - \bar{x} \right)^2 \right] \sigma^2
= \left( \dfrac{1}{n} + \dfrac{\left(x_0 - \bar{x} \right)^2}{S_{xx}} \right) \sigma^2
\end{align*}
\end{proof}

# Ejercicio 3
Suponer que se ajusta un modelo RLS por MCO a las observaciones $(x_i , y_i), i = 1, \ldots , n$. Verificar que se cumplen las siguientes igualdades:

Para los siguientes incisos, será necesario tener las siguientes identidades:
\begin{align*}
e_i &\stackrel{(i)}{=} y_i - \hat{y}_i & \hat{y}_i &\stackrel{(ii)}{=} \hat{\beta}_0 + \hat{\beta}_1 x_i & e_i &\stackrel{(iii)}{=} y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \\
\hat{\beta}_0 &\stackrel{(iv)}{=} \bar{y} - \dfrac{S_{xy}}{S_{xx}} \bar{x} & \hat{\beta}_1 &\stackrel{(v)}{=} \dfrac{S_{xy}}{S_{xx}} & \hat{\beta}_0 &\stackrel{(vi)}{=} \bar{y} - \hat{\beta}_1 \bar{x}
\end{align*}

## Inciso (a)
\begin{align}
\sum_{i = 1}^n e_i = 0
\end{align}

\begin{proof}
\begin{align*}
\sum_{i = 1}^n e_i
&\stackrel{(iii)}{=} \sum_{i = 1}^n \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right)
= \sum_{i = 1}^n y_i - \sum_{i = 1}^n \hat{\beta}_0 - \sum_{i = 1}^n \hat{\beta}_1 x_i
= n \bar{y} - n \hat{\beta}_0 - n \hat{\beta}_1 \bar{x} \\
&= n \left( \bar{y} - \hat{\beta}_0 - \hat{\beta}_1 \bar{x} \right) = 0 \qquad \text{usando la identidad } (vi)
\end{align*}
\end{proof}

## Inciso (b)
\begin{align}
\sum_{i = 1}^n x_i e_i = 0
\end{align}

\begin{proof}
\begin{align*}
\sum_{i = 1}^n x_i e_i
&\stackrel{(iii)}{=} \sum_{i = 1}^n x_i \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right)
= \sum_{i = 1}^n x_i y_i - \sum_{i = 1}^n x _i \hat{\beta}_0 - \sum_{i = 1}^n \hat{\beta}_1 x_i^2
= \sum_{i = 1}^n x_i y_i - n \hat{\beta}_0 \bar{x} - \hat{\beta}_1 \sum_{i = 1}^n x_i^2 \\
& \text{usando la identidad } (vi) \text{ se tiene} \\
&= \sum_{i = 1}^n x_i y_i - n \left(\bar{y} - \hat{\beta}_1 \bar{x} \right) \bar{x} - \hat{\beta}_1 \sum_{i = 1}^n x_i^2
= \sum_{i = 1}^n x_i y_i - n \bar{x} \bar{y} + n \hat{\beta}_1 \bar{x}^2 - \hat{\beta}_1 \sum_{i = 1}^n x_i^2 \\
&= \left( \sum_{i = 1}^n x_i y_i - n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i = 1}^n x_i^2 - n \bar{x}^2 \right) \\
& \text{ usando los incisos 6.(a), 6.(b) de la tarea 1 y la identidad } (v) \text{, se tiene} \\
&= S_{xy} - \dfrac{S_{xy}}{S_{xx}} S_{xx} = 0
\end{align*}
\end{proof}

## Inciso (c)
\begin{align}
\sum_{i = 1}^n \hat{y}_i e_i = 0
\end{align}

\begin{proof}
\begin{align*}
\sum_{i = 1}^n \hat{y}_i e_i
&\stackrel{(ii), (iii)}{=} \sum_{i = 1}^n \left[ \left( \hat{\beta}_0 + \hat{\beta}_1 x_i \right) \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right) \right]\\
&= \sum_{i = 1}^n \hat{\beta}_0 \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right) + \sum_{i = 1}^n \left[ \hat{\beta}_1 x_i \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right) \right] \\
&= \hat{\beta}_0 \left(\sum_{i = 1}^n y_i - \sum_{i = 1}^n \hat{\beta}_0 - \sum_{i = 1}^n \hat{\beta}_1 x_i \right) + \hat{\beta}_1 \left( \sum_{i = 1}^n x_i y_i - \sum_{i = 1}^n \hat{\beta}_0 x_i - \sum_{i = 1}^n \hat{\beta}_1 x_i^2 \right) \\
&= \hat{\beta}_0 \left(n \bar{y} - n \hat{\beta}_0 - n \hat{\beta}_1 \bar{x} \right) + \hat{\beta}_1 \left( \sum_{i = 1}^n x_i y_i - n \hat{\beta}_0 \bar{x} - \hat{\beta}_1 \sum_{i = 1}^n x_i^2 \right) \\
&= n \hat{\beta}_0 \left(\bar{y} - \hat{\beta}_0 - \hat{\beta}_1 \bar{x} \right) +
\hat{\beta}_1 \left( \sum_{i = 1}^n x_i y_i - n \hat{\beta}_0 \bar{x} - \hat{\beta}_1 \sum_{i = 1}^n x_i^2 \right) \\
& \text{por la identidad } (vi) \text{ se cancela el primer término } \\
& \text{y sustituimos el valor de } \hat{\beta}_0 \text{ en el segundo término} \\
&= n \hat{\beta}_0 \left(\cancelto{0}{\bar{y} - \hat{\beta}_0 - \hat{\beta}_1 \bar{x}} \right) +
\hat{\beta}_1 \left( \sum_{i = 1}^n x_i y_i - n \left(\bar{y} - \hat{\beta}_1 \bar{x} \right) \bar{x} - \hat{\beta}_1 \sum_{i = 1}^n x_i^2 \right) \\
&= \hat{\beta}_1 \left( \sum_{i = 1}^n x_i y_i - n \bar{x} \bar{y} + n \hat{\beta}_1 \bar{x}^2 - \hat{\beta}_1 \sum_{i = 1}^n x_i^2 \right) \\
&= \hat{\beta}_1 \left( \sum_{i = 1}^n x_i y_i - n \bar{x} \bar{y} \right) - \hat{\beta}_1^2 \left(\sum_{i = 1}^n x_i^2 - n \bar{x}^2 \right) \\
& \text{ usando los incisos 6.(a), 6.(b) de la tarea 1 y la identidad } (v) \text{, se tiene} \\
&= \dfrac{S_{xy}}{S_{xx}} \left( S_{xy} \right) - \dfrac{S_{xy}^2}{S_{xx}^2} \left(S_{xx} \right) = 0
\end{align*}
\end{proof}

## Inciso (d)
\begin{align}
\sum_{i = 1}^n \hat{y}_i = \sum_{i = 1}^n y_i
\end{align}

\begin{proof}
\begin{align*}
0 \stackrel{(a)}{=} \sum_{i = 1}^n e_i \stackrel{(i)}{=} \sum_{i = 1}^n \left(y_i -\hat{y}_i \right) = \sum_{i = 1}^n y_i - \sum_{i = 1}^n \hat{y}_i \text{, lo anterior implica que } \sum_{i = 1}^n \hat{y}_i = \sum_{i = 1}^n y_i
\end{align*}
\end{proof}

## Inciso (e)
\begin{align}
\sum_{i = 1}^n y_i e_i = \sum_{i = 1}^n e_i^2
\end{align}

\begin{proof}
\begin{align*}
\sum_{i = 1}^n y_i e_i
&\stackrel{(i)}{=} \sum_{i = 1}^n \left(e_i + \hat{y}_i \right) e_i
= \sum_{i = 1}^n e_i^2 + \sum_{i = 1}^n  \hat{y}_i e_i
\stackrel{(c)}{=} \sum_{i = 1}^n e_i^2 + \cancelto{0}{\sum_{i = 1}^n  \hat{y}_i e_i}
= \sum_{i = 1}^n e_i^2
\end{align*}
\end{proof}

## Inciso (f)
\begin{align}
\sum_{i = 1}^n (\hat{y}_i - \bar{y})^2 = \hat{\beta}_1^2 S_{xx}
\end{align}

\begin{proof}
\begin{align*}
\sum_{i = 1}^n (\hat{y}_i - \bar{y})^2
&= \sum_{i = 1}^n \left(\hat{y}_i^2 - 2 \hat{y}_i \bar{y} + \bar{y}^2 \right) 
= \sum_{i = 1}^n \hat{y}_i^2 - 2 \bar{y} \sum_{i = 1}^n \hat{y}_i + \sum_{i = 1}^n \bar{y}^2
\stackrel{(d)}{=} \sum_{i = 1}^n \hat{y}_i^2 - 2 \bar{y} \sum_{i = 1}^n y_i + \sum_{i = 1}^n \bar{y}^2 \\
&= \sum_{i = 1}^n \hat{y}_i^2 - 2 n \bar{y}^2 + n \bar{y}^2
= \sum_{i = 1}^n \hat{y}_i^2 - n \bar{y}^2
\stackrel{(ii)}{=} \sum_{i = 1}^n \left( \hat{\beta}_0 + \hat{\beta}_1 x_i \right)^2 - n \bar{y}^2 \\
&= \sum_{i = 1}^n \left( \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2 \right) - n \bar{y}^2 \\
&= \sum_{i = 1}^n \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 \sum_{i = 1}^n x_i + \hat{\beta}_1^2 \sum_{i = 1}^n x_i^2 - n \bar{y}^2 \\
&= n \hat{\beta}_0^2 + 2 n \hat{\beta}_0 \hat{\beta}_1 \bar{x} + \hat{\beta}_1^2 \sum_{i = 1}^n x_i^2 - n \bar{y}^2 \\
&\stackrel{(vi)}{=} n \left( \bar{y} - \hat{\beta}_1 \bar{x} \right)^2 + 2 n \left( \bar{y} - \hat{\beta}_1 \bar{x} \right) \hat{\beta}_1 \bar{x} + \hat{\beta}_1^2 \sum_{i = 1}^n x_i^2 - n \bar{y}^2 \\
&= n \left( \bar{y} - \hat{\beta}_1 \bar{x} \right)^2 + 2 n \left( \bar{y} - \hat{\beta}_1 \bar{x} \right) \hat{\beta}_1 \bar{x} + \hat{\beta}_1^2 \sum_{i = 1}^n x_i^2 - n \bar{y}^2 \\
&= n \bar{y}^2 - 2 n \hat{\beta}_1 \bar{x} \bar{y} + n \hat{\beta}_1^2 \bar{x}^2 + 2 n \hat{\beta}_1 \bar{x} \bar{y} - \hat{\beta}_1^2 \bar{x}^2 + \hat{\beta}_1^2 \sum_{i = 1}^n x_i^2 - n \bar{y}^2 \\
&= \hat{\beta}_1^2 \left( \sum_{i = 1}^n x_i^2 - \bar{x}^2 \right) = \hat{\beta}_1^2 S_{xx}, \qquad \text{usando el inciso 6.(a) de la tarea 1}
\end{align*}
\end{proof}

# Ejercicio 4
Calcular el error cuadrático medio de $\hat{\sigma}^2_{\mathrm{MCO}}$ y de $\hat{\sigma}^2_{\mathrm{MV}}$. A partir de los resultados decidir que estimador de $\sigma^2$ es mejor.

# Ejercicio 5
El conjunto de datos `mtcars` del paquete `datasets` de `R` contiene información sobre el rendimiento y otras características de $32$ vehículos. El rendimiento se encuentra en la variable `mpg` y está medido en millas por galón y el peso del vehículo está en la variable `wt` que está medida en miles de libras.


## Inciso (a)
Ajustar un modelo RLS para explicar `mpg` en términos de `wt`. Reportar las estimaciones de $\beta_0$ y $\beta_1$.

```{r, echo = TRUE}

# reutilizando la función definida en la tarea 1
rls <- function(x, y) {
  x.bar <- mean(x, na.rm = TRUE)
  y.bar <- mean(y, na.rm = TRUE)
  S.xx <- sum((x - x.bar)^2, na.rm = TRUE)
  S.xy <- sum((x - x.bar) * (y - y.bar), na.rm = TRUE)
  
  b0.hat <- y.bar - x.bar * S.xy / S.xx
  b1.hat <- S.xy / S.xx
  
  y.adj <- b0.hat + b1.hat * x
  residuos <- y - y.adj
  sigma2.hat <- sum(residuos^2) / (length(residuos) - 2)

  return(list(b0.hat = b0.hat, b1.hat = b1.hat, sigma2.hat = sigma2.hat, sxx = S.xx))
}

rls(mtcars$wt, mtcars$mpg)

```

Los valores estimados de $\beta_0$ y $\beta_1$ son los siguentes:

```{r, echo = TRUE}

paste(rls(mtcars$wt, mtcars$mpg)$b0.hat, " y ", rls(mtcars$wt, mtcars$mpg)$b1.hat)

```

## Inciso (b)
Transformar la variable `wt` a toneladas y repetir el inciso anterior. ¿Cómo se relacionan estas estimaciones de $\beta_0$ y $\beta_1$ con las anteriores?

Según la información de la base, la unidad de _wt_ es 1000 libras. Por lo que para transformala a toneladas basta multiplicarla por $0.453592$

```{r, echo = TRUE}

mtcars$wt_ton <- 0.453592 * mtcars$wt

```

Ajustando una regresión lineal, los valores estimados de $\beta_0$ y $\beta_1$ son los siguentes:

```{r, echo = TRUE}

paste(rls(mtcars$wt_ton, mtcars$mpg)$b0.hat, " y ", rls(mtcars$wt_ton, mtcars$mpg)$b1.hat)

```
En comparación con el modelo 1, notamos que este modelo tiene el mismo intercepto y su valor estimado para $\beta_1$ es $0.453592$ veces el valor de la $\beta_1$ del primer modelo.

## Inciso (c)
Repetir el inciso anterior pero ahora con `mpg` transformada a kilómetros por litro y `wt` en las unidades originales (miles de libras).

Según la información de la base, _mpg_ representa las millas por galón. Para transformar el rendimiento a kilómetros por litro, usaremos la siguiente información: una milla es $1.609344$kilómetros y un galón es $4.54609$ litros. Por lo que basta multiplicarlo por $0.35400618 = 1.609344 / 4.54609$.

```{r, echo = TRUE}

    mtcars$kpl <- 0.35400618 * mtcars$mpg

```

Ajustando una regresión lineal para el rendimiento medido por kilómetros por litros y peso en miles de libras, los valores estimados de $\beta_0$ y $\beta_1$ son los siguentes:

```{r, echo = TRUE}

paste(rls(mtcars$wt, mtcars$kpl)$b0.hat, " y ", rls(mtcars$wt, mtcars$kpl)$b1.hat)

```
En comparación con el modelo 1, notamos que este modelo tiene valores distintos tanto para $\beta_0$ como para $\beta_1$. En ambos casos estas nuevas estimaciones corresponden a la $0.3500618$ parte de las primeras estimaciones.


## Inciso (d)
Concluir sobre el efecto de los cambios de escala en las estimaciones de los parámetros del modelo RLS.

Si en un modelo RLS la variable independiente cambia de escala, la nueva estimación del intercepto no cambiará mientras que la de $\beta_1$ sí se modifica y lo hace en la misma razón en la que cambia la escala. En el caso del cambio de escala de la varible dependiente ambas estimaciones se modifican (su nuevo valor es exactamente al viejo valor multiplicado por el cambio de escala). En el primer caso (cambio de escala de variable independiente), cuando el valor de la variable independiente es cero (y tiene sentido interpretarla), la interpretación para la variable dependiente es la misma que sin cambio de escala; no obstante, la interpretación del cambio ante una variación en la variable independiente se modifica en razón del cambio de escala (pero en realidad podría decirse que es la misma). En el caso en que se modifica la escala de la variable dependiente, ambas estimaciones se modifican pero nuevamente este cambio sólo refleja el cambio de escala en los valores estimados.

# Ejercicio 6
Con el conjunto de datos `mtcars` ajustar un modelo RLS para explicar `mpg` en términos de `wt`

```{r, echo = TRUE}
rls(mtcars$wt, mtcars$mpg)
```

## Inciso (a)
Reportar las estimaciones de $V(\hat{\beta}_0)$ y $V(\hat{\beta}_1)$.

### Beta 1
Primero veamos que ocurre $V(\hat{\beta}_1)$. 

\begin{align}\label{eq:beta1}
   V(\hat{\beta}_1) = V(\frac{ \sum_{i = 1}^n(x_i - \bar{x})y_i }{ \sum_{i = 1}^n(x_i - \bar{x})^2}) = \frac{ 1 }{ \left[\sum_{i = 1}^n(x_i - \bar{x})^2 \right]^2 }
    \sum_{i = 1}^n(x_i - \bar{x})^2 {V} (y_i) = \frac{ \sigma^2 }{ \sum_{i = 1}^n(x_i - \bar{x})^2  }
\end{align}

La primera igualdad está dada por la definición de $\beta_1$ como la razón entre $S_{xy}$ y  $S_{xx}$. La segunda por las propiedades de la varianza y porque $V(\sum_{i = 1}^n(y_i))$ puede verse de la siguiente manera:

\begin{align}
V \left( \sum_{i = 1}^n y_i \right) = V \left( \sum_{i = 1}^n \beta_0 + \beta_1 X_i + \epsilon_i \right) = V \left( \sum_{i = 1}^n \epsilon_i \right) &= \sum_{i = 1}^n \sum_{j = 1}^n  Cov (\epsilon_i, \epsilon_j)  = \sum_{i = 1}^n  Cov (\epsilon_i, \epsilon_i) \sum_{i = 1}^n  V(\epsilon_i)\\
&= \sum_{i = 1}^n  V(\beta_0 + \beta_1 X_i + \epsilon_i)
= \sum_{i = 1}^n  V(y_i)\\
\end{align}

En este caso usamos la forma funcional que define a nuestro modelo, las propiedades de la varianza sobre constantes y el supuesto de que los errores son independientes (es decir, que $Cov (\epsilon_i, \epsilon_j)  = 0$)


### Beta 0
En el caso de $\beta_0$, podemos escribirlo en términos de $\beta1$ según la ecuación \ref{eq:beta1}. De tal manera, ocurre lo siguiente:

\begin{align*}\label{eq:beta0a}
  V(\hat{\beta_0}) = V(\bar{y}-\hat{\beta_1}\bar{x}) = V(\hat{\beta_0}) =       
  V(\bar{y})+(\bar{x}^2)V(\bar{\beta_1})- 2(\bar{x})Cov(\bar{Y},\hat{\beta_0}) 
\end{align*}

El primer término de la ecuación \ref{eq:beta0a} a su vez puede desarrollarse como sigue:
\begin{align*}
V(\bar{y})= V \left(\frac{1}{n} \sum_{i = 1}^n y_i \right)
  = \frac{1}{n^2} \sum_{i = 1}^n V(y_i)
  = \frac{\sigma^2}{n}
\end{align*}

El segundo término de \ref{eq:beta0a} ya se desarrolló anteriormente en \ref{eq:beta1}. Finalmente, la covarianza entre $\bar{y}$ y $\hat{\beta_1}$ puede verse como sigue: 

\begin{align}
  Cov (\bar{Y}, \hat{\beta}_1) &= Cov \left\{\frac{1}{n} \sum_{i = 1}^n Y_i,
     \frac{ \sum_{j = 1}^n(x_j - \bar{x})Y_j }{ \sum_{i = 1}^n(x_i - \bar{x})^2 }
     \right \} &= \frac{1}{n} \frac{ 1 }{ \sum_{i = 1}^n(x_i - \bar{x})^2 }
     Cov \left\{ \sum_{i = 1}^n Y_i, \sum_{j = 1}^n(x_j - \bar{x})Y_j \right\} \\
 &= \frac{ 1 }{ n \sum_{i = 1}^n(x_i - \bar{x})^2 }
    \sum_{i = 1}^n (x_j - \bar{x}) \sum_{j = 1}^n Cov(Y_i, Y_j) \\
 &= \frac{ 1 }{ n \sum_{i = 1}^n(x_i - \bar{x})^2 }
    \sum_{i = 1}^n (x_j - \bar{x}) \sigma^2 = 0
\end{align}

Esto último ocurre pues $\sum_{i = 1}^n (x_j - \bar{x})=0$

Así
\begin{align}
  V(\hat{\beta}_0)
 &= \frac{\sigma^2}{n} + \frac{ \sigma^2 \bar{x}^2}{ \sum_{i = 1}^n(x_i - \bar{x})^2  } \\
 &= \frac{\sigma^2 }{ n \sum_{i = 1}^n(x_i - \bar{x})^2 }
    \left\{ \sum_{i = 1}^n(x_i - \bar{x})^2 + n \bar{x}^2 \right\} \\
 &= \frac{\sigma^2 \sum_{i = 1}^n x_i^2}{ n \sum_{i = 1}^n(x_i - \bar{x})^2 } \label{eq:beta0}
\end{align}

### Estimación de Varianza 

Dadas las expresiones anteriores podemos calcular la varianza (y el inciso siguiente) de los estimadores con los siguientes valores:

```{r, echo = TRUE}
beta0 <- rls(mtcars$wt, mtcars$mpg)$b0.hat
beta1 <- rls(mtcars$wt, mtcars$mpg)$b1.hat
sigma2 <- rls(mtcars$wt, mtcars$mpg)$sigma2.hat
sxx <- rls(mtcars$wt, mtcars$mpg)$sxx
n <- nrow(mtcars) # pues no hay valores perdidos
sum_x2 <- sum((mtcars$wt)^2)
```

$\hat\beta_0 =$ `r beta0`,$\\$
$\hat\beta_1 =$ `r beta1`,$\\$
$\hat\sigma^2 =$ `r sigma2`,$\\$
$S_xx =$ `r sxx`,$\\$
$n$ = `r n`, $\\$
$\sum_i=1^n{x_i^2} =$ `r sum_x2`.

Así, sustituyendo estos datos del modelo ajustado en las expresiones dadas en las ecuaciones \ref{eq:beta0} y \ref{eq:beta1} encontramos que las varianzas de los estimadores del modelo son las siguientes:

```{r, echo = TRUE}
varBeta0 = (sigma2*sum_x2)/(n*sxx)
varBeta1 = sigma2/sxx
```
Así, $V(\beta_0) =$ `r varBeta0` y $V(\beta_1) =$ `r varBeta1`.

## Inciso (b)
Reportar la estimación de la medida del rendimiento de un vehículo con peso de $4300$ libras.

Para un coche con $4.3$ miles de libras, la medida de rendimiento estimada está dada por `r beta0 + beta1*4.3`$\frac{miles}{gallon}$.

## Inciso (c)
Reportar la estimación de la varianza de la estimación del inciso anterior.

Según lo demostrado en el ejercicio 2, sabemos que la varianza de la estimación media es la siguiente:

\begin{align}
  V(\hat{\mu}_0) = \sigma^2 \left(\dfrac{1}{n} + \dfrac{\left(x_0 - \bar{x}
  \right)^2}{S_{xx}} \right)
\end{align}

```{r, echo = TRUE}
varMu0 = sigma2*((1/n)+((14.3039 - mean(mtcars$wt))/sxx)^2)
```

Así, $V(\hat\mu_0) =$ `r varMu0`.